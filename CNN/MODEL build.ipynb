{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('twitter_labeled_v2.json', lines =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66035"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "favorite       0\n",
       "language       0\n",
       "magnitude    331\n",
       "polarity       0\n",
       "reply          0\n",
       "retweet        0\n",
       "score        331\n",
       "text           0\n",
       "time           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tranform label to binary vector [1,0,0] [0,1,0] [0,0,1] neg neural pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data.score[data.score.isnull()].index)# delete rows that api don't give the result\n",
    "#(in this data 331 twitter don't have result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65704"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### score < -0.5 strong negative; score < 0 and score >= -0.5 negative; score == 0 neutral; score > 0 and score <= 0.5 positive; score > 0.5 strong positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_value(score):\n",
    "    if score < -0.5:\n",
    "        return 0\n",
    "    if score < 0 and score >= -0.5:\n",
    "        return 1\n",
    "    if score == 0:\n",
    "        return 2\n",
    "    if score > 0 and score <= 0.5:\n",
    "        return 3\n",
    "    if score > 0.5:\n",
    "        return 4\n",
    "data['label_value'] = data['score'].apply(get_label_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('final_twitter.json', 'w') as f:\n",
    "    f.write(data.to_json(orient='records', lines=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "y = label_binarize(data['label_value'], classes=[0, 1, 2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(data['label_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x26a475082b0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.label_value.hist()# the data is not balance and most of twitters are positive or neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without emnbedding use tfidf vector as input nodes build a fully connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D,Conv1D, MaxPooling1D, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "tf_vec = tfidf.fit_transform(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27639"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vec.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_vec, y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52563, 27639), (52563, 5))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build first nerual network\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Dense(128, activation = 'relu', input_shape = (tf_vec.shape[1],)))\n",
    "\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(64, activation = 'relu'))\n",
    "model1.add(Dense(5, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               3537920   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 3,546,501\n",
      "Trainable params: 3,546,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='rmsprop', \n",
    "         loss='binary_crossentropy', \n",
    "         metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = X_train[:2000]\n",
    "partial_x_train = X_train[2000:]\n",
    "y_val = y_train[:2000]\n",
    "partial_y_train = y_train[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50563 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "50563/50563 [==============================] - 19s 370us/step - loss: 0.4653 - acc: 0.8082 - val_loss: 0.3902 - val_acc: 0.8291\n",
      "Epoch 2/20\n",
      "50563/50563 [==============================] - 18s 361us/step - loss: 0.3450 - acc: 0.8458 - val_loss: 0.3435 - val_acc: 0.8425\n",
      "Epoch 3/20\n",
      "50563/50563 [==============================] - 18s 361us/step - loss: 0.3005 - acc: 0.8671 - val_loss: 0.3281 - val_acc: 0.8498\n",
      "Epoch 4/20\n",
      "50563/50563 [==============================] - 18s 362us/step - loss: 0.2713 - acc: 0.8819 - val_loss: 0.3222 - val_acc: 0.8537\n",
      "Epoch 5/20\n",
      "50563/50563 [==============================] - 18s 359us/step - loss: 0.2471 - acc: 0.8941 - val_loss: 0.3213 - val_acc: 0.8562\n",
      "Epoch 6/20\n",
      "50563/50563 [==============================] - 18s 361us/step - loss: 0.2265 - acc: 0.9041 - val_loss: 0.3239 - val_acc: 0.8591\n",
      "Epoch 7/20\n",
      "50563/50563 [==============================] - 18s 360us/step - loss: 0.2076 - acc: 0.9141 - val_loss: 0.3290 - val_acc: 0.8595\n",
      "Epoch 8/20\n",
      "50563/50563 [==============================] - 18s 357us/step - loss: 0.1905 - acc: 0.9220 - val_loss: 0.3407 - val_acc: 0.8564\n",
      "Epoch 9/20\n",
      "50563/50563 [==============================] - 18s 357us/step - loss: 0.1759 - acc: 0.9296 - val_loss: 0.3469 - val_acc: 0.8555\n",
      "Epoch 10/20\n",
      "50563/50563 [==============================] - 18s 357us/step - loss: 0.1627 - acc: 0.9352 - val_loss: 0.3545 - val_acc: 0.8572\n",
      "Epoch 11/20\n",
      "50563/50563 [==============================] - 18s 356us/step - loss: 0.1496 - acc: 0.9412 - val_loss: 0.3637 - val_acc: 0.8549\n",
      "Epoch 12/20\n",
      "50563/50563 [==============================] - 18s 354us/step - loss: 0.1374 - acc: 0.9474 - val_loss: 0.3781 - val_acc: 0.8565\n",
      "Epoch 13/20\n",
      "50563/50563 [==============================] - 18s 356us/step - loss: 0.1268 - acc: 0.9517 - val_loss: 0.3905 - val_acc: 0.8554\n",
      "Epoch 14/20\n",
      "50563/50563 [==============================] - 18s 364us/step - loss: 0.1167 - acc: 0.9564 - val_loss: 0.4047 - val_acc: 0.8547\n",
      "Epoch 15/20\n",
      "50563/50563 [==============================] - 18s 358us/step - loss: 0.1068 - acc: 0.9607 - val_loss: 0.4201 - val_acc: 0.8566\n",
      "Epoch 16/20\n",
      "50563/50563 [==============================] - 18s 356us/step - loss: 0.0975 - acc: 0.9646 - val_loss: 0.4390 - val_acc: 0.8540\n",
      "Epoch 17/20\n",
      "50563/50563 [==============================] - 18s 357us/step - loss: 0.0895 - acc: 0.9682 - val_loss: 0.4514 - val_acc: 0.8534\n",
      "Epoch 18/20\n",
      "50563/50563 [==============================] - 18s 357us/step - loss: 0.0814 - acc: 0.9716 - val_loss: 0.4641 - val_acc: 0.8523\n",
      "Epoch 19/20\n",
      "50563/50563 [==============================] - 18s 356us/step - loss: 0.0747 - acc: 0.9743 - val_loss: 0.4871 - val_acc: 0.8543\n",
      "Epoch 20/20\n",
      "50563/50563 [==============================] - 18s 357us/step - loss: 0.0679 - acc: 0.9768 - val_loss: 0.5107 - val_acc: 0.8535\n"
     ]
    }
   ],
   "source": [
    "result = model1.fit(partial_x_train, partial_y_train, epochs=20,\n",
    "                   batch_size = 512, validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85001140537621922"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(X_test, y_test, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### amazing!  the orignial model i use 30000 twitter , labeld from paralleldots api(only give pos neg neu) the result is 80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build  a CNN model with embedding, padding layers, using keras tokenizer get input nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keras tokenlzer: fit_on_texts(data.text):get vocabulary list with each index\n",
    "#####tokenizer.texts_to_sequences: rebuild each sentence with word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data.text)\n",
    "sequences = tokenizer.texts_to_sequences(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'waves community launches \"guess the seed\" competition win 500 waves tokens!'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1402, 176, 1685, 658, 1, 5630, 1566, 119, 199, 1402, 54]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'to': 2,\n",
       " 'and': 3,\n",
       " 'is': 4,\n",
       " 'a': 5,\n",
       " '…': 6,\n",
       " 'for': 7,\n",
       " 'you': 8,\n",
       " 'in': 9,\n",
       " 'of': 10,\n",
       " 'i': 11,\n",
       " 'on': 12,\n",
       " 'it': 13,\n",
       " 'this': 14,\n",
       " 'get': 15,\n",
       " 'will': 16,\n",
       " 'be': 17,\n",
       " 'your': 18,\n",
       " 'now': 19,\n",
       " 'twitter': 20,\n",
       " 'with': 21,\n",
       " 'com': 22,\n",
       " 'free': 23,\n",
       " 'are': 24,\n",
       " 'that': 25,\n",
       " 'up': 26,\n",
       " 'at': 27,\n",
       " 'buy': 28,\n",
       " 'all': 29,\n",
       " 'more': 30,\n",
       " 'verge': 31,\n",
       " 'we': 32,\n",
       " 'join': 33,\n",
       " 'my': 34,\n",
       " 'if': 35,\n",
       " 'have': 36,\n",
       " 'pic': 37,\n",
       " 'just': 38,\n",
       " 'so': 39,\n",
       " 'coin': 40,\n",
       " 'not': 41,\n",
       " 'here': 42,\n",
       " 'time': 43,\n",
       " 'new': 44,\n",
       " 'what': 45,\n",
       " 'like': 46,\n",
       " 'crypto': 47,\n",
       " 'but': 48,\n",
       " 'out': 49,\n",
       " 'can': 50,\n",
       " 'has': 51,\n",
       " 'about': 52,\n",
       " 'do': 53,\n",
       " 'tokens': 54,\n",
       " 'people': 55,\n",
       " 'as': 56,\n",
       " 'coins': 57,\n",
       " 'me': 58,\n",
       " 'or': 59,\n",
       " 'xvg': 60,\n",
       " 'from': 61,\n",
       " 'bonus': 62,\n",
       " 'when': 63,\n",
       " 'some': 64,\n",
       " '1': 65,\n",
       " 'go': 66,\n",
       " 'they': 67,\n",
       " 'going': 68,\n",
       " 'good': 69,\n",
       " 'no': 70,\n",
       " 'see': 71,\n",
       " 'by': 72,\n",
       " '100': 73,\n",
       " 'pre': 74,\n",
       " 'token': 75,\n",
       " '0': 76,\n",
       " 'one': 77,\n",
       " \"it's\": 78,\n",
       " 'price': 79,\n",
       " '2018': 80,\n",
       " 'its': 81,\n",
       " 'best': 82,\n",
       " 'sell': 83,\n",
       " '2': 84,\n",
       " 'our': 85,\n",
       " 'link': 86,\n",
       " 'still': 87,\n",
       " 'an': 88,\n",
       " 'how': 89,\n",
       " 'moon': 90,\n",
       " 'binance': 91,\n",
       " 'only': 92,\n",
       " 'was': 93,\n",
       " \"don't\": 94,\n",
       " '50': 95,\n",
       " 'bitcoin': 96,\n",
       " 'there': 97,\n",
       " 'great': 98,\n",
       " 'registration': 99,\n",
       " 'today': 100,\n",
       " 'next': 101,\n",
       " 'make': 102,\n",
       " 'ico': 103,\n",
       " 'market': 104,\n",
       " '300': 105,\n",
       " 'follow': 106,\n",
       " 'first': 107,\n",
       " 'pump': 108,\n",
       " 'trading': 109,\n",
       " 'their': 110,\n",
       " 'think': 111,\n",
       " 'share': 112,\n",
       " 'wraith': 113,\n",
       " 'before': 114,\n",
       " 'who': 115,\n",
       " 'sale': 116,\n",
       " 'money': 117,\n",
       " 'back': 118,\n",
       " 'win': 119,\n",
       " 'hold': 120,\n",
       " 'know': 121,\n",
       " 'airdrop': 122,\n",
       " 'big': 123,\n",
       " 'right': 124,\n",
       " 'would': 125,\n",
       " 'why': 126,\n",
       " '3': 127,\n",
       " 'soon': 128,\n",
       " 'please': 129,\n",
       " 'then': 130,\n",
       " 'exchange': 131,\n",
       " 'news': 132,\n",
       " 'day': 133,\n",
       " 'earn': 134,\n",
       " 'register': 135,\n",
       " 'again': 136,\n",
       " 'closed': 137,\n",
       " 'need': 138,\n",
       " 'guys': 139,\n",
       " 'want': 140,\n",
       " 'over': 141,\n",
       " 'year': 142,\n",
       " 'trade': 143,\n",
       " 'down': 144,\n",
       " 'been': 145,\n",
       " \"i'm\": 146,\n",
       " 'very': 147,\n",
       " 'than': 148,\n",
       " 'retweet': 149,\n",
       " 'started': 150,\n",
       " 'privacy': 151,\n",
       " 'take': 152,\n",
       " 'us': 153,\n",
       " 'team': 154,\n",
       " 'coming': 155,\n",
       " 'days': 156,\n",
       " 'quick': 157,\n",
       " 'hodl': 158,\n",
       " '10': 159,\n",
       " 'future': 160,\n",
       " 'short': 161,\n",
       " 'many': 162,\n",
       " 'last': 163,\n",
       " 'group': 164,\n",
       " '000': 165,\n",
       " 'should': 166,\n",
       " 'wallet': 167,\n",
       " 'after': 168,\n",
       " 'blockchain': 169,\n",
       " 'into': 170,\n",
       " 'telegram': 171,\n",
       " 'each': 172,\n",
       " '5': 173,\n",
       " 'off': 174,\n",
       " 'much': 175,\n",
       " 'community': 176,\n",
       " 'cryptocurrency': 177,\n",
       " 'sign': 178,\n",
       " 'any': 179,\n",
       " 'keep': 180,\n",
       " 'way': 181,\n",
       " 'use': 182,\n",
       " 'refer': 183,\n",
       " 'everyone': 184,\n",
       " 'check': 185,\n",
       " 'believe': 186,\n",
       " 'offer': 187,\n",
       " 'which': 188,\n",
       " 'long': 189,\n",
       " 'let': 190,\n",
       " 'look': 191,\n",
       " 'even': 192,\n",
       " 'these': 193,\n",
       " 'other': 194,\n",
       " 'member': 195,\n",
       " 'own': 196,\n",
       " 'them': 197,\n",
       " 'am': 198,\n",
       " '500': 199,\n",
       " 'ref': 200,\n",
       " 'too': 201,\n",
       " 'btc': 202,\n",
       " 'got': 203,\n",
       " 'dip': 204,\n",
       " 'start': 205,\n",
       " 'low': 206,\n",
       " 'bought': 207,\n",
       " 'valid': 208,\n",
       " 'week': 209,\n",
       " 'love': 210,\n",
       " 'because': 211,\n",
       " 'come': 212,\n",
       " 'looking': 213,\n",
       " 'protocol': 214,\n",
       " 'could': 215,\n",
       " 'another': 216,\n",
       " 'project': 217,\n",
       " 'better': 218,\n",
       " 'he': 219,\n",
       " 'while': 220,\n",
       " 'chance': 221,\n",
       " 'account': 222,\n",
       " 'nice': 223,\n",
       " 'most': 224,\n",
       " 'few': 225,\n",
       " 'huge': 226,\n",
       " 'invest': 227,\n",
       " 'well': 228,\n",
       " 'fud': 229,\n",
       " '4': 230,\n",
       " 'end': 231,\n",
       " 'profit': 232,\n",
       " 'really': 233,\n",
       " 'those': 234,\n",
       " 'currency': 235,\n",
       " 'top': 236,\n",
       " 'ready': 237,\n",
       " 'hours': 238,\n",
       " 'friends': 239,\n",
       " 'world': 240,\n",
       " 'help': 241,\n",
       " 'also': 242,\n",
       " 'give': 243,\n",
       " 'tweet': 244,\n",
       " 'real': 245,\n",
       " 'thanks': 246,\n",
       " 'others': 247,\n",
       " 'where': 248,\n",
       " 'already': 249,\n",
       " 'stop': 250,\n",
       " 'hope': 251,\n",
       " 'via': 252,\n",
       " 'release': 253,\n",
       " 'don’t': 254,\n",
       " 'rt': 255,\n",
       " 'reach': 256,\n",
       " 'miss': 257,\n",
       " 'worth': 258,\n",
       " 'did': 259,\n",
       " 'open': 260,\n",
       " 'work': 261,\n",
       " 'buying': 262,\n",
       " 'strong': 263,\n",
       " 'never': 264,\n",
       " 'wait': 265,\n",
       " '20': 266,\n",
       " 'say': 267,\n",
       " 'released': 268,\n",
       " 'rise': 269,\n",
       " 'made': 270,\n",
       " 'holding': 271,\n",
       " 'u': 272,\n",
       " 'support': 273,\n",
       " 'receive': 274,\n",
       " '1000': 275,\n",
       " 'getting': 276,\n",
       " 'lot': 277,\n",
       " 'i’m': 278,\n",
       " 'create': 279,\n",
       " 'ripple': 280,\n",
       " 'being': 281,\n",
       " 'sure': 282,\n",
       " 'looks': 283,\n",
       " 'dont': 284,\n",
       " 'every': 285,\n",
       " 'had': 286,\n",
       " 'live': 287,\n",
       " \"let's\": 288,\n",
       " 'hit': 289,\n",
       " 'hard': 290,\n",
       " 'lol': 291,\n",
       " 'around': 292,\n",
       " 'doing': 293,\n",
       " 'cheap': 294,\n",
       " 'bring': 295,\n",
       " 'sold': 296,\n",
       " 'anyone': 297,\n",
       " 'something': 298,\n",
       " 'fast': 299,\n",
       " 'waiting': 300,\n",
       " 'selling': 301,\n",
       " 'available': 302,\n",
       " 'friend': 303,\n",
       " 'read': 304,\n",
       " 'his': 305,\n",
       " 'working': 306,\n",
       " 'same': 307,\n",
       " 'things': 308,\n",
       " 'term': 309,\n",
       " 'tron': 310,\n",
       " 'thank': 311,\n",
       " 'thing': 312,\n",
       " 'run': 313,\n",
       " 'potential': 314,\n",
       " 'address': 315,\n",
       " 'dump': 316,\n",
       " 'vote': 317,\n",
       " 'official': 318,\n",
       " 'happy': 319,\n",
       " 'mining': 320,\n",
       " 'away': 321,\n",
       " 'watch': 322,\n",
       " 'fake': 323,\n",
       " 'giveaway': 324,\n",
       " 'projects': 325,\n",
       " 'lets': 326,\n",
       " 'travel': 327,\n",
       " 'marketplace': 328,\n",
       " 'high': 329,\n",
       " 'secured': 330,\n",
       " 'giving': 331,\n",
       " 'forex': 332,\n",
       " 'sport': 333,\n",
       " 'signals': 334,\n",
       " 'pumptoken': 335,\n",
       " 'tourism': 336,\n",
       " 'volume': 337,\n",
       " 'always': 338,\n",
       " 'machtcoin': 339,\n",
       " 'gonna': 340,\n",
       " 'below': 341,\n",
       " 'investment': 342,\n",
       " \"can't\": 343,\n",
       " 'kucoin': 344,\n",
       " 'does': 345,\n",
       " 'serenity': 346,\n",
       " 'try': 347,\n",
       " 'airdrops': 348,\n",
       " 'move': 349,\n",
       " 'sats': 350,\n",
       " 'goal': 351,\n",
       " 'done': 352,\n",
       " 'stay': 353,\n",
       " 'since': 354,\n",
       " '6': 355,\n",
       " 'once': 356,\n",
       " '15': 357,\n",
       " 'making': 358,\n",
       " 'shit': 359,\n",
       " 'feel': 360,\n",
       " 'followers': 361,\n",
       " 'little': 362,\n",
       " 'users': 363,\n",
       " 'drop': 364,\n",
       " 'yet': 365,\n",
       " '30': 366,\n",
       " 'nothing': 367,\n",
       " 'said': 368,\n",
       " 'through': 369,\n",
       " 'exchanges': 370,\n",
       " 'put': 371,\n",
       " 'everything': 372,\n",
       " 'planning': 373,\n",
       " 'month': 374,\n",
       " 'true': 375,\n",
       " '200': 376,\n",
       " 'less': 377,\n",
       " 'email': 378,\n",
       " 'website': 379,\n",
       " 'months': 380,\n",
       " 'send': 381,\n",
       " 'video': 382,\n",
       " 'bullshit': 383,\n",
       " 'value': 384,\n",
       " 'fees': 385,\n",
       " 'panic': 386,\n",
       " 'ever': 387,\n",
       " 'cap': 388,\n",
       " 'scam': 389,\n",
       " 'someone': 390,\n",
       " 'until': 391,\n",
       " 'late': 392,\n",
       " 'both': 393,\n",
       " 'green': 394,\n",
       " \"you're\": 395,\n",
       " 'trying': 396,\n",
       " 'private': 397,\n",
       " 'easy': 398,\n",
       " 'amazing': 399,\n",
       " 'yes': 400,\n",
       " 'using': 401,\n",
       " 'ago': 402,\n",
       " 'tech': 403,\n",
       " 'were': 404,\n",
       " 'break': 405,\n",
       " 'post': 406,\n",
       " 'life': 407,\n",
       " 'update': 408,\n",
       " 'trx': 409,\n",
       " 'investors': 410,\n",
       " 'goes': 411,\n",
       " 'may': 412,\n",
       " 'eth': 413,\n",
       " 'left': 414,\n",
       " '24': 415,\n",
       " 'hurry': 416,\n",
       " \"that's\": 417,\n",
       " 'remember': 418,\n",
       " 'gains': 419,\n",
       " 'january': 420,\n",
       " 'code': 421,\n",
       " 'winner': 422,\n",
       " 'maybe': 423,\n",
       " 'man': 424,\n",
       " 'portfolio': 425,\n",
       " 'years': 426,\n",
       " 'bad': 427,\n",
       " 'xrp': 428,\n",
       " 'target': 429,\n",
       " 'million': 430,\n",
       " 'find': 431,\n",
       " 'coinbase': 432,\n",
       " 'chart': 433,\n",
       " '25': 434,\n",
       " 'profits': 435,\n",
       " 'almost': 436,\n",
       " 'tomorrow': 437,\n",
       " 'finally': 438,\n",
       " 'far': 439,\n",
       " 'im': 440,\n",
       " 'business': 441,\n",
       " 'seems': 442,\n",
       " 'enter': 443,\n",
       " 'pool': 444,\n",
       " 'must': 445,\n",
       " 'signup': 446,\n",
       " 'whales': 447,\n",
       " 'moment': 448,\n",
       " 'technology': 449,\n",
       " 'tell': 450,\n",
       " 'weeks': 451,\n",
       " 'channel': 452,\n",
       " 'transactions': 453,\n",
       " 'add': 454,\n",
       " 'game': 455,\n",
       " 'two': 456,\n",
       " 'might': 457,\n",
       " 'undervalued': 458,\n",
       " 'cents': 459,\n",
       " 'click': 460,\n",
       " 'current': 461,\n",
       " 'product': 462,\n",
       " 'announcement': 463,\n",
       " 'article': 464,\n",
       " 'show': 465,\n",
       " 'comment': 466,\n",
       " 'hey': 467,\n",
       " 'change': 468,\n",
       " 'happen': 469,\n",
       " \"i've\": 470,\n",
       " 'rocket': 471,\n",
       " 'fee': 472,\n",
       " 'trust': 473,\n",
       " '2017': 474,\n",
       " 'anything': 475,\n",
       " 'lose': 476,\n",
       " 'members': 477,\n",
       " 'ride': 478,\n",
       " 'part': 479,\n",
       " 'listed': 480,\n",
       " 'under': 481,\n",
       " 'starting': 482,\n",
       " 'dev': 483,\n",
       " '12': 484,\n",
       " 'night': 485,\n",
       " 'referral': 486,\n",
       " 'early': 487,\n",
       " 'research': 488,\n",
       " 'awesome': 489,\n",
       " 'hands': 490,\n",
       " 'network': 491,\n",
       " 'updates': 492,\n",
       " 'currently': 493,\n",
       " 'usd': 494,\n",
       " 'payment': 495,\n",
       " 'taking': 496,\n",
       " '7': 497,\n",
       " 'actually': 498,\n",
       " 'understand': 499,\n",
       " 'following': 500,\n",
       " 'yourself': 501,\n",
       " 'platform': 502,\n",
       " 'later': 503,\n",
       " 'point': 504,\n",
       " 'prices': 505,\n",
       " 'cryptos': 506,\n",
       " 'else': 507,\n",
       " 'forget': 508,\n",
       " 'small': 509,\n",
       " 'behind': 510,\n",
       " 'hodling': 511,\n",
       " 'without': 512,\n",
       " 'hour': 513,\n",
       " 'transaction': 514,\n",
       " 'tweets': 515,\n",
       " 'marketing': 516,\n",
       " 'past': 517,\n",
       " 'investing': 518,\n",
       " 'train': 519,\n",
       " 'grow': 520,\n",
       " 'analysis': 521,\n",
       " 'biggest': 522,\n",
       " 'dips': 523,\n",
       " 'times': 524,\n",
       " 'signal': 525,\n",
       " '8': 526,\n",
       " 'stealth': 527,\n",
       " 'anonymous': 528,\n",
       " 'such': 529,\n",
       " 'march': 530,\n",
       " 'morning': 531,\n",
       " 'yesterday': 532,\n",
       " 'list': 533,\n",
       " 'jump': 534,\n",
       " 'needs': 535,\n",
       " 'enjoy': 536,\n",
       " 'daily': 537,\n",
       " 'opportunity': 538,\n",
       " 'd': 539,\n",
       " 'guy': 540,\n",
       " 'invested': 541,\n",
       " 'spread': 542,\n",
       " 'expect': 543,\n",
       " 'smart': 544,\n",
       " \"what's\": 545,\n",
       " 'full': 546,\n",
       " \"i'll\": 547,\n",
       " 'makes': 548,\n",
       " 'satoshi': 549,\n",
       " 'launch': 550,\n",
       " 'holders': 551,\n",
       " 'pay': 552,\n",
       " 'supply': 553,\n",
       " 'lost': 554,\n",
       " 'together': 555,\n",
       " 'grab': 556,\n",
       " 'missed': 557,\n",
       " 'christmas': 558,\n",
       " 'wanna': 559,\n",
       " 'till': 560,\n",
       " 'enough': 561,\n",
       " 'added': 562,\n",
       " 'gain': 563,\n",
       " 'within': 564,\n",
       " 'red': 565,\n",
       " 'watching': 566,\n",
       " 'learn': 567,\n",
       " 'comes': 568,\n",
       " 'means': 569,\n",
       " 'possible': 570,\n",
       " 'mcafee': 571,\n",
       " 'bit': 572,\n",
       " '1st': 573,\n",
       " 'upcoin': 574,\n",
       " 'place': 575,\n",
       " 'sat': 576,\n",
       " 'wow': 577,\n",
       " 'set': 578,\n",
       " 'fuck': 579,\n",
       " 'call': 580,\n",
       " 'word': 581,\n",
       " 'accepting': 582,\n",
       " 'cryptocurrencies': 583,\n",
       " 'info': 584,\n",
       " 'crazy': 585,\n",
       " 'weak': 586,\n",
       " 'step': 587,\n",
       " 'growing': 588,\n",
       " 'hype': 589,\n",
       " 'interesting': 590,\n",
       " 'growth': 591,\n",
       " 'happening': 592,\n",
       " 'wrong': 593,\n",
       " 'oh': 594,\n",
       " 'asset': 595,\n",
       " 'saying': 596,\n",
       " 'altcoins': 597,\n",
       " 'bullish': 598,\n",
       " 'talking': 599,\n",
       " 'having': 600,\n",
       " 'faith': 601,\n",
       " 'rich': 602,\n",
       " 'reason': 603,\n",
       " 'credit': 604,\n",
       " 'minutes': 605,\n",
       " 'seen': 606,\n",
       " 'works': 607,\n",
       " 'fork': 608,\n",
       " 'gets': 609,\n",
       " 'correction': 610,\n",
       " 'luck': 611,\n",
       " \"doesn't\": 612,\n",
       " '9': 613,\n",
       " 'massive': 614,\n",
       " 'pretty': 615,\n",
       " 'called': 616,\n",
       " 'charts': 617,\n",
       " 'claim': 618,\n",
       " 'cash': 619,\n",
       " 'prediction': 620,\n",
       " \"didn't\": 621,\n",
       " 'become': 622,\n",
       " 'wall': 623,\n",
       " 'says': 624,\n",
       " 's': 625,\n",
       " 'advice': 626,\n",
       " 'public': 627,\n",
       " 'tonight': 628,\n",
       " 'him': 629,\n",
       " 'haters': 630,\n",
       " 'increase': 631,\n",
       " 'though': 632,\n",
       " 'job': 633,\n",
       " 'hits': 634,\n",
       " 'global': 635,\n",
       " 'bottom': 636,\n",
       " 'forward': 637,\n",
       " 'starts': 638,\n",
       " 'least': 639,\n",
       " 'based': 640,\n",
       " 'choice': 641,\n",
       " 'mine': 642,\n",
       " 'whale': 643,\n",
       " '40': 644,\n",
       " 'site': 645,\n",
       " 'thoughts': 646,\n",
       " 'accounts': 647,\n",
       " 'talk': 648,\n",
       " '11': 649,\n",
       " '00': 650,\n",
       " 'power': 651,\n",
       " \"we're\": 652,\n",
       " 'hate': 653,\n",
       " 'due': 654,\n",
       " 'moving': 655,\n",
       " 'amount': 656,\n",
       " 'jan': 657,\n",
       " 'guess': 658,\n",
       " 'stock': 659,\n",
       " 'whole': 660,\n",
       " 'told': 661,\n",
       " 'close': 662,\n",
       " 'matter': 663,\n",
       " 'thought': 664,\n",
       " 'breakout': 665,\n",
       " 'bags': 666,\n",
       " 'went': 667,\n",
       " 'higher': 668,\n",
       " 'between': 669,\n",
       " 'can’t': 670,\n",
       " 'yeah': 671,\n",
       " 'ok': 672,\n",
       " 'information': 673,\n",
       " 'per': 674,\n",
       " 'created': 675,\n",
       " \"won't\": 676,\n",
       " 'loss': 677,\n",
       " 'agree': 678,\n",
       " 'alt': 679,\n",
       " 'feeling': 680,\n",
       " 'tip': 681,\n",
       " 'reopened': 682,\n",
       " 'bigger': 683,\n",
       " 'bittrex': 684,\n",
       " 'github': 685,\n",
       " 'instead': 686,\n",
       " 'loans': 687,\n",
       " 'simply': 688,\n",
       " 'mean': 689,\n",
       " 'patience': 690,\n",
       " 'wants': 691,\n",
       " 'wish': 692,\n",
       " 'sorry': 693,\n",
       " 'fucking': 694,\n",
       " 'source': 695,\n",
       " 'major': 696,\n",
       " '14': 697,\n",
       " 'idea': 698,\n",
       " 'hacked': 699,\n",
       " 'couple': 700,\n",
       " 'accept': 701,\n",
       " 'etc': 702,\n",
       " 'wave': 703,\n",
       " 'lots': 704,\n",
       " 'altcoin': 705,\n",
       " 'second': 706,\n",
       " 'john': 707,\n",
       " 'order': 708,\n",
       " 'seeing': 709,\n",
       " 'family': 710,\n",
       " 'dollar': 711,\n",
       " 'definitely': 712,\n",
       " 'company': 713,\n",
       " 'used': 714,\n",
       " 'devs': 715,\n",
       " '…pic': 716,\n",
       " 'genieico': 717,\n",
       " 'important': 718,\n",
       " 'x': 719,\n",
       " 'pr': 720,\n",
       " 'billion': 721,\n",
       " 'opinion': 722,\n",
       " 'hodlers': 723,\n",
       " 'app': 724,\n",
       " 'person': 725,\n",
       " 'happened': 726,\n",
       " 'welcome': 727,\n",
       " 'boom': 728,\n",
       " 'able': 729,\n",
       " 'fly': 730,\n",
       " '01': 731,\n",
       " 'thinking': 732,\n",
       " 'you’re': 733,\n",
       " 'december': 734,\n",
       " 'different': 735,\n",
       " 'glad': 736,\n",
       " 'thats': 737,\n",
       " 'trend': 738,\n",
       " 'ask': 739,\n",
       " 'bull': 740,\n",
       " 'regret': 741,\n",
       " 'bank': 742,\n",
       " 'against': 743,\n",
       " 'ethereum': 744,\n",
       " 'found': 745,\n",
       " 'beginning': 746,\n",
       " 'serious': 747,\n",
       " 'limited': 748,\n",
       " 'registrations': 749,\n",
       " 'hitbtc': 750,\n",
       " 'during': 751,\n",
       " 'probably': 752,\n",
       " 'card': 753,\n",
       " 'bitrace': 754,\n",
       " 'roadmap': 755,\n",
       " 'windows': 756,\n",
       " 'dm': 757,\n",
       " 'simple': 758,\n",
       " 'secure': 759,\n",
       " 'discord': 760,\n",
       " 'folks': 761,\n",
       " 'bag': 762,\n",
       " 'positive': 763,\n",
       " 'decentralized': 764,\n",
       " 'fall': 765,\n",
       " 'crash': 766,\n",
       " 're': 767,\n",
       " 'knows': 768,\n",
       " 'level': 769,\n",
       " 'above': 770,\n",
       " 'baby': 771,\n",
       " 'safe': 772,\n",
       " 'leave': 773,\n",
       " 'solid': 774,\n",
       " 'tor': 775,\n",
       " 'walls': 776,\n",
       " 'proof': 777,\n",
       " 'position': 778,\n",
       " 'care': 779,\n",
       " 'form': 780,\n",
       " 'south': 781,\n",
       " 'korea': 782,\n",
       " 'markets': 783,\n",
       " 'cause': 784,\n",
       " 'interested': 785,\n",
       " 'page': 786,\n",
       " 'case': 787,\n",
       " 'currencies': 788,\n",
       " 'fun': 789,\n",
       " 'took': 790,\n",
       " 'complete': 791,\n",
       " 'fill': 792,\n",
       " '18': 793,\n",
       " 'rising': 794,\n",
       " 'quite': 795,\n",
       " 'pick': 796,\n",
       " 'wallets': 797,\n",
       " 'line': 798,\n",
       " 'dollars': 799,\n",
       " 'download': 800,\n",
       " 'traders': 801,\n",
       " 'mind': 802,\n",
       " 'media': 803,\n",
       " 'excited': 804,\n",
       " 'action': 805,\n",
       " 'hopefully': 806,\n",
       " 'pumps': 807,\n",
       " \"isn't\": 808,\n",
       " 'name': 809,\n",
       " 'announced': 810,\n",
       " 'wake': 811,\n",
       " 'play': 812,\n",
       " 'winners': 813,\n",
       " 'korean': 814,\n",
       " '2000': 815,\n",
       " 'i’ll': 816,\n",
       " 'heard': 817,\n",
       " 'ahead': 818,\n",
       " 'double': 819,\n",
       " 'recent': 820,\n",
       " 'sleep': 821,\n",
       " '16': 822,\n",
       " 'push': 823,\n",
       " 'bounce': 824,\n",
       " 'continue': 825,\n",
       " 'perfect': 826,\n",
       " 'i’ve': 827,\n",
       " '22': 828,\n",
       " 'blocks': 829,\n",
       " 'easily': 830,\n",
       " 'total': 831,\n",
       " 'alts': 832,\n",
       " 'near': 833,\n",
       " 'guide': 834,\n",
       " 'takes': 835,\n",
       " 'half': 836,\n",
       " 'latest': 837,\n",
       " 'choose': 838,\n",
       " 'happens': 839,\n",
       " 'social': 840,\n",
       " 'number': 841,\n",
       " 'favorite': 842,\n",
       " 'option': 843,\n",
       " '13': 844,\n",
       " 'raise': 845,\n",
       " 'everybody': 846,\n",
       " 'lucky': 847,\n",
       " 'tips': 848,\n",
       " 'fully': 849,\n",
       " 'lower': 850,\n",
       " 'vs': 851,\n",
       " 'question': 852,\n",
       " 'developer': 853,\n",
       " 'payments': 854,\n",
       " 'patient': 855,\n",
       " 'predictions': 856,\n",
       " 'transfer': 857,\n",
       " 'single': 858,\n",
       " 'litecoin': 859,\n",
       " 'pumping': 860,\n",
       " 'listen': 861,\n",
       " 'system': 862,\n",
       " 'banks': 863,\n",
       " 'miner': 864,\n",
       " 'block': 865,\n",
       " 'monero': 866,\n",
       " 'fear': 867,\n",
       " '24h': 868,\n",
       " 'calm': 869,\n",
       " 'store': 870,\n",
       " 'problem': 871,\n",
       " 'discount': 872,\n",
       " 'ppl': 873,\n",
       " \"haven't\": 874,\n",
       " 'visit': 875,\n",
       " 'ones': 876,\n",
       " 'super': 877,\n",
       " '70': 878,\n",
       " '–': 879,\n",
       " 'exactly': 880,\n",
       " 'steps': 881,\n",
       " 'promising': 882,\n",
       " 'risk': 883,\n",
       " 'adoption': 884,\n",
       " 'lambo': 885,\n",
       " 'clear': 886,\n",
       " 'hoping': 887,\n",
       " 'old': 888,\n",
       " '05': 889,\n",
       " 'ath': 890,\n",
       " '150': 891,\n",
       " 'running': 892,\n",
       " 'resistance': 893,\n",
       " '21': 894,\n",
       " '60': 895,\n",
       " \"'s\": 896,\n",
       " 'spreading': 897,\n",
       " 'ledger': 898,\n",
       " 'digital': 899,\n",
       " 'confirmed': 900,\n",
       " 'technical': 901,\n",
       " 'cant': 902,\n",
       " 'whats': 903,\n",
       " \"they're\": 904,\n",
       " 'anonymity': 905,\n",
       " 'online': 906,\n",
       " 'answer': 907,\n",
       " 'security': 908,\n",
       " 'key': 909,\n",
       " 'mark': 910,\n",
       " 'expected': 911,\n",
       " 'deal': 912,\n",
       " 'shows': 913,\n",
       " 'either': 914,\n",
       " 'n': 915,\n",
       " 'stupid': 916,\n",
       " 'hell': 917,\n",
       " 'date': 918,\n",
       " 'doubt': 919,\n",
       " 'feb': 920,\n",
       " 'february': 921,\n",
       " 'load': 922,\n",
       " 'experience': 923,\n",
       " 'fomo': 924,\n",
       " 'financial': 925,\n",
       " 'course': 926,\n",
       " 'stuff': 927,\n",
       " 'held': 928,\n",
       " 'history': 929,\n",
       " 'space': 930,\n",
       " 'upcoming': 931,\n",
       " 'attention': 932,\n",
       " '23': 933,\n",
       " 'wi': 934,\n",
       " 'fi': 935,\n",
       " 'faster': 936,\n",
       " 'consider': 937,\n",
       " '17': 938,\n",
       " 'truly': 939,\n",
       " 'ur': 940,\n",
       " 'congrats': 941,\n",
       " 'participate': 942,\n",
       " 'however': 943,\n",
       " 'kind': 944,\n",
       " 'rest': 945,\n",
       " 'addresses': 946,\n",
       " 'legit': 947,\n",
       " 'iungo': 948,\n",
       " 'dead': 949,\n",
       " 'gone': 950,\n",
       " 'developers': 951,\n",
       " 'ban': 952,\n",
       " 'dude': 953,\n",
       " 'wanted': 954,\n",
       " 'track': 955,\n",
       " 'dumping': 956,\n",
       " 'fact': 957,\n",
       " 'speed': 958,\n",
       " 'bet': 959,\n",
       " 'main': 960,\n",
       " 'plan': 961,\n",
       " 'user': 962,\n",
       " '800': 963,\n",
       " 'success': 964,\n",
       " 'words': 965,\n",
       " 'build': 966,\n",
       " 'especially': 967,\n",
       " 'paid': 968,\n",
       " 'dropping': 969,\n",
       " 'cool': 970,\n",
       " 'story': 971,\n",
       " 'asking': 972,\n",
       " 'companies': 973,\n",
       " 'r': 974,\n",
       " 'cryptopia': 975,\n",
       " 'alot': 976,\n",
       " 'damn': 977,\n",
       " 'hear': 978,\n",
       " 'poll': 979,\n",
       " 'god': 980,\n",
       " '02': 981,\n",
       " 'ip': 982,\n",
       " 'movement': 983,\n",
       " 'worry': 984,\n",
       " 'steady': 985,\n",
       " 'millions': 986,\n",
       " 'thug': 987,\n",
       " '”': 988,\n",
       " 'fam': 989,\n",
       " 'paper': 990,\n",
       " 'imagine': 991,\n",
       " 'trades': 992,\n",
       " 'entry': 993,\n",
       " 'weekend': 994,\n",
       " 'active': 995,\n",
       " 'vip': 996,\n",
       " 'myself': 997,\n",
       " 'p': 998,\n",
       " 'updated': 999,\n",
       " 'breaking': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(x) for x in sequences])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the highest word number in a twitter list is 65. pad other sentence with 0 value to make every sentence have same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_model2 = pad_sequences(sequences, maxlen=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_model2, y, test_size=0.1, random_state=2)\n",
    "partial_x_train, x_val, partial_y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200#set word vector'length\n",
    "word_count = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 65, 200)           5864200   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 65, 200)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 63, 250)           150250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 21, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 5250)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 200)               1050200   \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 5)                 1005      \n",
      "=================================================================\n",
      "Total params: 7,065,655\n",
      "Trainable params: 7,065,655\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(word_count + 1, embedding_dim, input_length=65))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Conv1D(250, 3, padding='valid', activation='relu'))#convoultion layer filters = 250 kernel_size = 3 \n",
    "#one layer convolution,because the max word is 60\n",
    "model2.add(MaxPooling1D(3))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(200, activation='relu'))\n",
    "model2.add(Dense(5, activation='softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53219 samples, validate on 5914 samples\n",
      "Epoch 1/20\n",
      "53219/53219 [==============================] - 51s 960us/step - loss: 0.3900 - acc: 0.8288 - val_loss: 0.3218 - val_acc: 0.8540\n",
      "Epoch 2/20\n",
      "53219/53219 [==============================] - 51s 960us/step - loss: 0.2971 - acc: 0.8648 - val_loss: 0.2928 - val_acc: 0.8661\n",
      "Epoch 3/20\n",
      "53219/53219 [==============================] - 51s 967us/step - loss: 0.2554 - acc: 0.8870 - val_loss: 0.2891 - val_acc: 0.8682\n",
      "Epoch 4/20\n",
      "53219/53219 [==============================] - 51s 963us/step - loss: 0.2241 - acc: 0.9041 - val_loss: 0.3008 - val_acc: 0.8660\n",
      "Epoch 5/20\n",
      "53219/53219 [==============================] - 50s 948us/step - loss: 0.1956 - acc: 0.9187 - val_loss: 0.3068 - val_acc: 0.8647\n",
      "Epoch 6/20\n",
      "53219/53219 [==============================] - 51s 949us/step - loss: 0.1674 - acc: 0.9320 - val_loss: 0.3332 - val_acc: 0.8571\n",
      "Epoch 7/20\n",
      "53219/53219 [==============================] - 51s 952us/step - loss: 0.1391 - acc: 0.9457 - val_loss: 0.3373 - val_acc: 0.8637\n",
      "Epoch 8/20\n",
      "53219/53219 [==============================] - 51s 957us/step - loss: 0.1148 - acc: 0.9557 - val_loss: 0.3679 - val_acc: 0.8595\n",
      "Epoch 9/20\n",
      "53219/53219 [==============================] - 51s 951us/step - loss: 0.0944 - acc: 0.9648 - val_loss: 0.3969 - val_acc: 0.8581\n",
      "Epoch 10/20\n",
      "53219/53219 [==============================] - 51s 965us/step - loss: 0.0749 - acc: 0.9725 - val_loss: 0.4346 - val_acc: 0.8561\n",
      "Epoch 11/20\n",
      "53219/53219 [==============================] - 53s 989us/step - loss: 0.0604 - acc: 0.9783 - val_loss: 0.4618 - val_acc: 0.8561\n",
      "Epoch 12/20\n",
      "53219/53219 [==============================] - 51s 953us/step - loss: 0.0492 - acc: 0.9826 - val_loss: 0.5064 - val_acc: 0.8494\n",
      "Epoch 13/20\n",
      "53219/53219 [==============================] - 52s 977us/step - loss: 0.0391 - acc: 0.9863 - val_loss: 0.5292 - val_acc: 0.8535\n",
      "Epoch 14/20\n",
      "53219/53219 [==============================] - 51s 955us/step - loss: 0.0323 - acc: 0.9889 - val_loss: 0.6110 - val_acc: 0.8456\n",
      "Epoch 15/20\n",
      "53219/53219 [==============================] - 50s 946us/step - loss: 0.0261 - acc: 0.9911 - val_loss: 0.6233 - val_acc: 0.8473\n",
      "Epoch 16/20\n",
      "53219/53219 [==============================] - 51s 950us/step - loss: 0.0226 - acc: 0.9926 - val_loss: 0.6733 - val_acc: 0.8515\n",
      "Epoch 17/20\n",
      "53219/53219 [==============================] - 53s 989us/step - loss: 0.0189 - acc: 0.9938 - val_loss: 0.6924 - val_acc: 0.8486\n",
      "Epoch 18/20\n",
      "53219/53219 [==============================] - 53s 987us/step - loss: 0.0168 - acc: 0.9947 - val_loss: 0.7387 - val_acc: 0.8508\n",
      "Epoch 19/20\n",
      "53219/53219 [==============================] - 53s 990us/step - loss: 0.0143 - acc: 0.9955 - val_loss: 0.7577 - val_acc: 0.8464\n",
      "Epoch 20/20\n",
      "53219/53219 [==============================] - 52s 982us/step - loss: 0.0133 - acc: 0.9958 - val_loss: 0.7772 - val_acc: 0.8478\n"
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer='rmsprop', \n",
    "         loss='binary_crossentropy', \n",
    "         metrics=['acc'])\n",
    "result2 = model2.fit(partial_x_train, partial_y_train, epochs=20,\n",
    "                   batch_size = 512, validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84486377577570504"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_test, y_test, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combined wordvec2 with cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### because I build two wordvec2 model based on have stop words or not,so I should build a embedding layers by my self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### single word wordvec2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEMP\\Anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv2single = gensim.models.word2vec.Word2Vec.load(\"singleword2v.w2v\")\n",
    "wv2_single_voc = list(wv2single.wv.vocab.keys())\n",
    "wv2_single_voc_dict = dict(zip(wv2_single_voc,list(range(len(wv2_single_voc)))))\n",
    "wv2_single_sequences = []\n",
    "for text in data.text:\n",
    "    temp_vec = []\n",
    "    for word in text.split():\n",
    "        if word in wv2_single_voc:\n",
    "            temp_vec.append(wv2_single_voc_dict[word])\n",
    "    wv2_single_sequences.append(temp_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 0.8452621102333069),\n",
       " ('nice', 0.7789426445960999),\n",
       " ('forward', 0.7583998441696167),\n",
       " ('bad', 0.6935279369354248),\n",
       " ('amazing', 0.6809878349304199),\n",
       " ('positive', 0.656152069568634),\n",
       " ('awesome', 0.6407425403594971),\n",
       " ('better', 0.639358401298523),\n",
       " ('pretty', 0.632267951965332),\n",
       " ('big', 0.6230407953262329)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv2single.wv.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6646"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wv2_single_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65704"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wv2_single_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(a) for a in wv2_single_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x210c17e9080>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series([len(a) for a in wv2_single_sequences]).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_model3 = pad_sequences(wv2_single_sequences, maxlen=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(wv2_single_voc_dict)+ 1, 100))\n",
    "for word,index in wv2_single_voc_dict.items():\n",
    "    embedding_matrix[index] = wv2single.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6647, 100)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(wv2_single_voc_dict)+ 1,\n",
    "                            100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=45,\n",
    "                            trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_model3, y, test_size=0.1, random_state=2)\n",
    "partial_x_train, x_val, partial_y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 45, 100)           664700    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 45, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 43, 250)           75250     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 10750)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               1075100   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 1,815,555\n",
      "Trainable params: 1,150,855\n",
      "Non-trainable params: 664,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(embedding_layer)\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(Conv1D(250, 3, padding='valid', activation='relu'))#convoultion layer filters = 250 kernel_size = 3 \n",
    "#one layer convolution,because the max word is too samll\n",
    "#model3.add(MaxPooling1D(3))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(100, activation='relu'))\n",
    "model3.add(Dense(5, activation='softmax'))\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53219 samples, validate on 5914 samples\n",
      "Epoch 1/20\n",
      "53219/53219 [==============================] - 16s 297us/step - loss: 0.4439 - acc: 0.8151 - val_loss: 0.3857 - val_acc: 0.8284\n",
      "Epoch 2/20\n",
      "53219/53219 [==============================] - 16s 293us/step - loss: 0.3834 - acc: 0.8287 - val_loss: 0.3766 - val_acc: 0.8305\n",
      "Epoch 3/20\n",
      "53219/53219 [==============================] - 16s 291us/step - loss: 0.3708 - acc: 0.8346 - val_loss: 0.3729 - val_acc: 0.8327\n",
      "Epoch 4/20\n",
      "53219/53219 [==============================] - 15s 289us/step - loss: 0.3604 - acc: 0.8399 - val_loss: 0.3684 - val_acc: 0.8368\n",
      "Epoch 5/20\n",
      "53219/53219 [==============================] - 15s 291us/step - loss: 0.3513 - acc: 0.8440 - val_loss: 0.3695 - val_acc: 0.8334\n",
      "Epoch 6/20\n",
      "53219/53219 [==============================] - 15s 291us/step - loss: 0.3422 - acc: 0.8486 - val_loss: 0.3717 - val_acc: 0.8360\n",
      "Epoch 7/20\n",
      "53219/53219 [==============================] - 16s 295us/step - loss: 0.3335 - acc: 0.8531 - val_loss: 0.3683 - val_acc: 0.8378\n",
      "Epoch 8/20\n",
      "53219/53219 [==============================] - 15s 290us/step - loss: 0.3245 - acc: 0.8574 - val_loss: 0.3717 - val_acc: 0.8376\n",
      "Epoch 9/20\n",
      "53219/53219 [==============================] - 15s 289us/step - loss: 0.3161 - acc: 0.8621 - val_loss: 0.3739 - val_acc: 0.8369\n",
      "Epoch 10/20\n",
      "53219/53219 [==============================] - 15s 290us/step - loss: 0.3070 - acc: 0.8665 - val_loss: 0.3783 - val_acc: 0.8354\n",
      "Epoch 11/20\n",
      "53219/53219 [==============================] - 16s 292us/step - loss: 0.2975 - acc: 0.8708 - val_loss: 0.3941 - val_acc: 0.8330\n",
      "Epoch 12/20\n",
      "53219/53219 [==============================] - 15s 291us/step - loss: 0.2886 - acc: 0.8755 - val_loss: 0.3966 - val_acc: 0.8330\n",
      "Epoch 13/20\n",
      "53219/53219 [==============================] - 15s 290us/step - loss: 0.2806 - acc: 0.8789 - val_loss: 0.3991 - val_acc: 0.8329\n",
      "Epoch 14/20\n",
      "53219/53219 [==============================] - 15s 289us/step - loss: 0.2721 - acc: 0.8832 - val_loss: 0.4047 - val_acc: 0.8298\n",
      "Epoch 15/20\n",
      "53219/53219 [==============================] - 15s 290us/step - loss: 0.2639 - acc: 0.8872 - val_loss: 0.4079 - val_acc: 0.8310\n",
      "Epoch 16/20\n",
      "53219/53219 [==============================] - 15s 289us/step - loss: 0.2557 - acc: 0.8915 - val_loss: 0.4172 - val_acc: 0.8318\n",
      "Epoch 17/20\n",
      "53219/53219 [==============================] - 15s 291us/step - loss: 0.2486 - acc: 0.8946 - val_loss: 0.4339 - val_acc: 0.8286\n",
      "Epoch 18/20\n",
      "53219/53219 [==============================] - 15s 290us/step - loss: 0.2413 - acc: 0.8979 - val_loss: 0.4431 - val_acc: 0.8285\n",
      "Epoch 19/20\n",
      "53219/53219 [==============================] - 15s 289us/step - loss: 0.2348 - acc: 0.9014 - val_loss: 0.4540 - val_acc: 0.8251\n",
      "Epoch 20/20\n",
      "53219/53219 [==============================] - 15s 291us/step - loss: 0.2291 - acc: 0.9037 - val_loss: 0.4591 - val_acc: 0.8263\n"
     ]
    }
   ],
   "source": [
    "model3.compile(optimizer='rmsprop', \n",
    "         loss='binary_crossentropy', \n",
    "         metrics=['acc'])\n",
    "result3 = model3.fit(partial_x_train, partial_y_train, epochs=20,\n",
    "                   batch_size = 512, validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82319282579233322"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(X_test, y_test, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use a single word model that have stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv2single = gensim.models.word2vec.Word2Vec.load(\"nofilter.w2v\")\n",
    "wv2_single_voc = list(wv2single.wv.vocab.keys())\n",
    "wv2_single_voc_dict = dict(zip(wv2_single_voc,list(range(len(wv2_single_voc)))))\n",
    "wv2_single_sequences = []\n",
    "for text in data.text:\n",
    "    temp_vec = []\n",
    "    \n",
    "    for word in text.split():\n",
    "        if word in wv2_single_voc:\n",
    "            temp_vec.append(wv2_single_voc_dict[word])\n",
    "    wv2_single_sequences.append(temp_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8992"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wv2_single_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(a) for a in wv2_single_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_model3 = pad_sequences(wv2_single_sequences, maxlen=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(wv2_single_voc_dict)+ 1, 200))\n",
    "for word,index in wv2_single_voc_dict.items():\n",
    "    embedding_matrix[index] = wv2single .wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8993, 200)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_model3, y, test_size=0.1, random_state=2)\n",
    "partial_x_train, x_val, partial_y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(wv2_single_voc_dict)+ 1,\n",
    "                            200,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=60,\n",
    "                            trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 60, 200)           1798600   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 60, 200)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 58, 250)           150250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 19, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 4750)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               475100    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 2,424,455\n",
      "Trainable params: 625,855\n",
      "Non-trainable params: 1,798,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(embedding_layer)\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(Conv1D(250, 3, padding='valid', activation='relu'))#convoultion layer filters = 250 kernel_size = 3 \n",
    "#one layer convolution,because the max word is too samll\n",
    "model3.add(MaxPooling1D(3))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(100, activation='relu'))\n",
    "model3.add(Dense(5, activation='softmax'))\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53219 samples, validate on 5914 samples\n",
      "Epoch 1/20\n",
      "53219/53219 [==============================] - 31s 577us/step - loss: 0.3380 - acc: 0.8502 - val_loss: 0.3545 - val_acc: 0.8447\n",
      "Epoch 2/20\n",
      "53219/53219 [==============================] - 31s 575us/step - loss: 0.3238 - acc: 0.8570 - val_loss: 0.3564 - val_acc: 0.8443\n",
      "Epoch 3/20\n",
      "53219/53219 [==============================] - 31s 574us/step - loss: 0.3117 - acc: 0.8624 - val_loss: 0.3556 - val_acc: 0.8458\n",
      "Epoch 4/20\n",
      "53219/53219 [==============================] - 31s 575us/step - loss: 0.2992 - acc: 0.8694 - val_loss: 0.3695 - val_acc: 0.8392\n",
      "Epoch 5/20\n",
      "53219/53219 [==============================] - 31s 575us/step - loss: 0.2869 - acc: 0.8755 - val_loss: 0.3741 - val_acc: 0.8414\n",
      "Epoch 6/20\n",
      "53219/53219 [==============================] - 31s 576us/step - loss: 0.2745 - acc: 0.8819 - val_loss: 0.3766 - val_acc: 0.8423\n",
      "Epoch 7/20\n",
      "53219/53219 [==============================] - 30s 573us/step - loss: 0.2629 - acc: 0.8875 - val_loss: 0.3881 - val_acc: 0.8358\n",
      "Epoch 8/20\n",
      "53219/53219 [==============================] - 31s 578us/step - loss: 0.2503 - acc: 0.8939 - val_loss: 0.4154 - val_acc: 0.8322\n",
      "Epoch 9/20\n",
      "53219/53219 [==============================] - 31s 575us/step - loss: 0.2395 - acc: 0.8995 - val_loss: 0.4096 - val_acc: 0.8323\n",
      "Epoch 10/20\n",
      "53219/53219 [==============================] - 31s 577us/step - loss: 0.2279 - acc: 0.9049 - val_loss: 0.4106 - val_acc: 0.8336\n",
      "Epoch 11/20\n",
      "53219/53219 [==============================] - 31s 575us/step - loss: 0.2174 - acc: 0.9100 - val_loss: 0.4239 - val_acc: 0.8348\n",
      "Epoch 12/20\n",
      "53219/53219 [==============================] - 31s 575us/step - loss: 0.2084 - acc: 0.9139 - val_loss: 0.4508 - val_acc: 0.8287\n",
      "Epoch 13/20\n",
      "53219/53219 [==============================] - 31s 574us/step - loss: 0.1990 - acc: 0.9182 - val_loss: 0.4523 - val_acc: 0.8302\n",
      "Epoch 14/20\n",
      "53219/53219 [==============================] - 31s 575us/step - loss: 0.1895 - acc: 0.9227 - val_loss: 0.4844 - val_acc: 0.8250\n",
      "Epoch 15/20\n",
      "53219/53219 [==============================] - 31s 577us/step - loss: 0.1807 - acc: 0.9262 - val_loss: 0.4803 - val_acc: 0.8281\n",
      "Epoch 16/20\n",
      "53219/53219 [==============================] - 31s 576us/step - loss: 0.1734 - acc: 0.9303 - val_loss: 0.5045 - val_acc: 0.8237\n",
      "Epoch 17/20\n",
      "53219/53219 [==============================] - 31s 576us/step - loss: 0.1666 - acc: 0.9331 - val_loss: 0.5217 - val_acc: 0.8235\n",
      "Epoch 18/20\n",
      "53219/53219 [==============================] - 31s 577us/step - loss: 0.1591 - acc: 0.9367 - val_loss: 0.5643 - val_acc: 0.8203\n",
      "Epoch 19/20\n",
      "53219/53219 [==============================] - 31s 575us/step - loss: 0.1520 - acc: 0.9395 - val_loss: 0.5576 - val_acc: 0.8211\n",
      "Epoch 20/20\n",
      "53219/53219 [==============================] - 31s 576us/step - loss: 0.1461 - acc: 0.9420 - val_loss: 0.5707 - val_acc: 0.8204\n"
     ]
    }
   ],
   "source": [
    "model3.compile(optimizer='rmsprop', \n",
    "         loss='binary_crossentropy', \n",
    "         metrics=['acc'])\n",
    "result3 = model3.fit(partial_x_train, partial_y_train, epochs=20,\n",
    "                   batch_size = 512, validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82051437503579294"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(X_test, y_test, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use pretrained glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "glove_twitter = api.load(\"glove-twitter-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEMP\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('great', 0.8672515749931335),\n",
       " ('well', 0.8378019332885742),\n",
       " ('nice', 0.8338046073913574),\n",
       " ('better', 0.8206877112388611),\n",
       " ('night', 0.8163402080535889),\n",
       " ('morning', 0.8079935908317566),\n",
       " ('but', 0.8020180463790894),\n",
       " ('bad', 0.7983508110046387),\n",
       " ('too', 0.7936956286430359),\n",
       " ('it', 0.7926609516143799)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_twitter.wv.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEMP\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1193514"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_twitter.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "for w in glove_twitter.vocab.keys():\n",
    "    embeddings_index[w] = glove_twitter[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_vec, y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data.text)\n",
    "sequences = tokenizer.texts_to_sequences(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29320"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)#tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelx6 = pad_sequences(sequences, maxlen=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         1402,   176,  1685,   658,     1,  5630,  1566,   119,   199,\n",
       "         1402,    54],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         2020, 13523,    65, 13524,    76, 13525,    76, 13526,    76,\n",
       "        13527,  1076,  9956,  1753, 13528,  4039, 13529,  8143, 13530,\n",
       "           84, 13531],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,  9957,   624,    25,    96, 13532,   201,\n",
       "          175,  5181],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "          134,   398,   254,   257,    49,   446,    12,   446,   252,\n",
       "           14,    86],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,   416,\n",
       "            2,   134,     5,   357,    62,    12,     1,    75,   116,\n",
       "          948,     1,   107,   635,   934,   935,   169,   491,    15,\n",
       "           62,    42]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelx6[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index)+1, 200))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29321, 200)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(modelx6, y, test_size=0.1, random_state=2)\n",
    "partial_x_train, x_val, partial_y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 65, 200)           5864200   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 65, 200)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 63, 250)           150250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 21, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 5250)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               525100    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 6,540,055\n",
      "Trainable params: 675,855\n",
      "Non-trainable params: 5,864,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model6 = Sequential()\n",
    "model6.add(Embedding(len(tokenizer.word_index)+1, 200, weights=[embedding_matrix], input_length=65, trainable=False))\n",
    "model6.add(Dropout(0.2))\n",
    "model6.add(Conv1D(250, 3, padding='valid', activation='relu'))#convoultion layer filters = 250 kernel_size = 3 \n",
    "#one layer convolution,because the max word is too samll\n",
    "model6.add(MaxPooling1D(3))\n",
    "model6.add(Flatten())\n",
    "model6.add(Dense(100, activation='relu'))\n",
    "model6.add(Dense(5, activation='softmax'))\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53219 samples, validate on 5914 samples\n",
      "Epoch 1/20\n",
      "53219/53219 [==============================] - 38s 710us/step - loss: 0.4187 - acc: 0.8220 - val_loss: 0.3452 - val_acc: 0.8414\n",
      "Epoch 2/20\n",
      "53219/53219 [==============================] - 38s 719us/step - loss: 0.3393 - acc: 0.8451 - val_loss: 0.3385 - val_acc: 0.8427\n",
      "Epoch 3/20\n",
      "53219/53219 [==============================] - 38s 719us/step - loss: 0.3184 - acc: 0.8554 - val_loss: 0.3178 - val_acc: 0.8550\n",
      "Epoch 4/20\n",
      "53219/53219 [==============================] - 38s 707us/step - loss: 0.3018 - acc: 0.8636 - val_loss: 0.3103 - val_acc: 0.8575\n",
      "Epoch 5/20\n",
      "53219/53219 [==============================] - 37s 702us/step - loss: 0.2868 - acc: 0.8716 - val_loss: 0.3208 - val_acc: 0.8504\n",
      "Epoch 6/20\n",
      "53219/53219 [==============================] - 37s 703us/step - loss: 0.2727 - acc: 0.8796 - val_loss: 0.3166 - val_acc: 0.8552\n",
      "Epoch 7/20\n",
      "53219/53219 [==============================] - 37s 703us/step - loss: 0.2584 - acc: 0.8877 - val_loss: 0.3166 - val_acc: 0.8550\n",
      "Epoch 8/20\n",
      "53219/53219 [==============================] - 38s 715us/step - loss: 0.2443 - acc: 0.8944 - val_loss: 0.3237 - val_acc: 0.8530\n",
      "Epoch 9/20\n",
      "53219/53219 [==============================] - 40s 751us/step - loss: 0.2317 - acc: 0.9006 - val_loss: 0.3282 - val_acc: 0.8530\n",
      "Epoch 10/20\n",
      "53219/53219 [==============================] - 38s 708us/step - loss: 0.2183 - acc: 0.9078 - val_loss: 0.3560 - val_acc: 0.8410\n",
      "Epoch 11/20\n",
      "53219/53219 [==============================] - 38s 707us/step - loss: 0.2081 - acc: 0.9134 - val_loss: 0.3519 - val_acc: 0.8509\n",
      "Epoch 12/20\n",
      "53219/53219 [==============================] - 37s 704us/step - loss: 0.1965 - acc: 0.9185 - val_loss: 0.3590 - val_acc: 0.8495\n",
      "Epoch 13/20\n",
      "53219/53219 [==============================] - 37s 704us/step - loss: 0.1877 - acc: 0.9226 - val_loss: 0.3552 - val_acc: 0.8523\n",
      "Epoch 14/20\n",
      "53219/53219 [==============================] - 37s 702us/step - loss: 0.1781 - acc: 0.9267 - val_loss: 0.3781 - val_acc: 0.8525\n",
      "Epoch 15/20\n",
      "53219/53219 [==============================] - 38s 714us/step - loss: 0.1710 - acc: 0.9302 - val_loss: 0.3959 - val_acc: 0.8443\n",
      "Epoch 16/20\n",
      "53219/53219 [==============================] - 38s 717us/step - loss: 0.1613 - acc: 0.9347 - val_loss: 0.3977 - val_acc: 0.8427\n",
      "Epoch 17/20\n",
      "53219/53219 [==============================] - 39s 738us/step - loss: 0.1546 - acc: 0.9378 - val_loss: 0.4157 - val_acc: 0.8451\n",
      "Epoch 18/20\n",
      "53219/53219 [==============================] - 38s 711us/step - loss: 0.1489 - acc: 0.9407 - val_loss: 0.4107 - val_acc: 0.8442\n",
      "Epoch 19/20\n",
      "53219/53219 [==============================] - 38s 716us/step - loss: 0.1410 - acc: 0.9437 - val_loss: 0.4129 - val_acc: 0.8470\n",
      "Epoch 20/20\n",
      "53219/53219 [==============================] - 38s 721us/step - loss: 0.1362 - acc: 0.9457 - val_loss: 0.4319 - val_acc: 0.8450\n"
     ]
    }
   ],
   "source": [
    "model6.compile(optimizer='rmsprop', \n",
    "         loss='binary_crossentropy', \n",
    "         metrics=['acc'])\n",
    "result6 = model6.fit(partial_x_train, partial_y_train, epochs=20,\n",
    "                   batch_size = 512, validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84611169444863388"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6.evaluate(X_test, y_test, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use bigram set conv1d kernel_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 65, 200)           5864200   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 65, 200)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 64, 250)           100250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 21, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 5250)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               525100    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 6,490,055\n",
      "Trainable params: 625,855\n",
      "Non-trainable params: 5,864,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model7 = Sequential()\n",
    "model7.add(Embedding(len(tokenizer.word_index)+1, 200, weights=[embedding_matrix], input_length=65, trainable=False))\n",
    "model7.add(Dropout(0.2))\n",
    "model7.add(Conv1D(250, 2, padding='valid', activation='relu'))#convoultion layer filters = 250 kernel_size = 3 \n",
    "#one layer convolution,because the max word is too samll\n",
    "model7.add(MaxPooling1D(3))\n",
    "model7.add(Flatten())\n",
    "model7.add(Dense(100, activation='relu'))\n",
    "model7.add(Dense(5, activation='softmax'))\n",
    "model7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53219 samples, validate on 5914 samples\n",
      "Epoch 1/10\n",
      "53219/53219 [==============================] - 35s 652us/step - loss: 0.0893 - acc: 0.9663 - val_loss: 0.6086 - val_acc: 0.8368\n",
      "Epoch 2/10\n",
      "53219/53219 [==============================] - 35s 665us/step - loss: 0.0774 - acc: 0.9711 - val_loss: 0.6240 - val_acc: 0.8399\n",
      "Epoch 3/10\n",
      "53219/53219 [==============================] - 34s 645us/step - loss: 0.0730 - acc: 0.9731 - val_loss: 0.6635 - val_acc: 0.8395\n",
      "Epoch 4/10\n",
      "53219/53219 [==============================] - 35s 649us/step - loss: 0.0685 - acc: 0.9752 - val_loss: 0.6329 - val_acc: 0.8370\n",
      "Epoch 5/10\n",
      "53219/53219 [==============================] - 35s 649us/step - loss: 0.0634 - acc: 0.9767 - val_loss: 0.6569 - val_acc: 0.8394\n",
      "Epoch 6/10\n",
      "53219/53219 [==============================] - 35s 650us/step - loss: 0.0612 - acc: 0.9778 - val_loss: 0.6610 - val_acc: 0.8363\n",
      "Epoch 7/10\n",
      "53219/53219 [==============================] - 35s 654us/step - loss: 0.0611 - acc: 0.9780 - val_loss: 0.6743 - val_acc: 0.8412\n",
      "Epoch 8/10\n",
      "53219/53219 [==============================] - 35s 649us/step - loss: 0.0566 - acc: 0.9800 - val_loss: 0.6898 - val_acc: 0.8368\n",
      "Epoch 9/10\n",
      "53219/53219 [==============================] - 35s 656us/step - loss: 0.0554 - acc: 0.9799 - val_loss: 0.7013 - val_acc: 0.8370\n",
      "Epoch 10/10\n",
      "53219/53219 [==============================] - 35s 660us/step - loss: 0.0533 - acc: 0.9807 - val_loss: 0.7037 - val_acc: 0.8397\n"
     ]
    }
   ],
   "source": [
    "model7.compile(optimizer='rmsprop', \n",
    "         loss='binary_crossentropy', \n",
    "         metrics=['acc'])\n",
    "result7 = model6.fit(partial_x_train, partial_y_train, epochs=10,\n",
    "                   batch_size = 512, validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80003050830951661"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7.evaluate(X_test, y_test, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add LSTM layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 65, 200)           5864200   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 1005      \n",
      "=================================================================\n",
      "Total params: 6,186,005\n",
      "Trainable params: 321,805\n",
      "Non-trainable params: 5,864,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model8 = Sequential()\n",
    "model8.add(Embedding(len(tokenizer.word_index)+1, 200, weights=[embedding_matrix], input_length=65, trainable=False))\n",
    "model8.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2))\n",
    "model8.add(Dropout(0.2))\n",
    "\n",
    "model8.add(Dense(5, activation='softmax'))\n",
    "model8.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53219 samples, validate on 5914 samples\n",
      "Epoch 1/10\n",
      "53219/53219 [==============================] - 35s 651us/step - loss: 0.0526 - acc: 0.9816 - val_loss: 0.7072 - val_acc: 0.8385\n",
      "Epoch 2/10\n",
      "53219/53219 [==============================] - 34s 640us/step - loss: 0.0507 - acc: 0.9818 - val_loss: 0.7415 - val_acc: 0.8333\n",
      "Epoch 3/10\n",
      "53219/53219 [==============================] - 34s 637us/step - loss: 0.0495 - acc: 0.9826 - val_loss: 0.7230 - val_acc: 0.8387\n",
      "Epoch 4/10\n",
      "53219/53219 [==============================] - 34s 639us/step - loss: 0.0495 - acc: 0.9823 - val_loss: 0.7353 - val_acc: 0.8378\n",
      "Epoch 5/10\n",
      "53219/53219 [==============================] - 34s 637us/step - loss: 0.0473 - acc: 0.9833 - val_loss: 0.7482 - val_acc: 0.8366\n",
      "Epoch 6/10\n",
      "53219/53219 [==============================] - 34s 640us/step - loss: 0.0458 - acc: 0.9839 - val_loss: 0.7569 - val_acc: 0.8374\n",
      "Epoch 7/10\n",
      "53219/53219 [==============================] - 34s 640us/step - loss: 0.0452 - acc: 0.9843 - val_loss: 0.7784 - val_acc: 0.8356\n",
      "Epoch 8/10\n",
      "53219/53219 [==============================] - 34s 641us/step - loss: 0.0442 - acc: 0.9847 - val_loss: 0.7823 - val_acc: 0.8357\n",
      "Epoch 9/10\n",
      "53219/53219 [==============================] - 34s 643us/step - loss: 0.0431 - acc: 0.9850 - val_loss: 0.8015 - val_acc: 0.8376\n",
      "Epoch 10/10\n",
      "53219/53219 [==============================] - 34s 635us/step - loss: 0.0429 - acc: 0.9848 - val_loss: 0.7827 - val_acc: 0.8362\n"
     ]
    }
   ],
   "source": [
    "model8.compile(optimizer='rmsprop', \n",
    "         loss='binary_crossentropy', \n",
    "         metrics=['acc'])\n",
    "result8 = model6.fit(partial_x_train, partial_y_train, epochs=10,\n",
    "                   batch_size = 512, validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80000007142579421"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model8.evaluate(X_test, y_test, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_model = gensim.models.Doc2Vec.load('doc2vec.w2v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65704"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_model.docvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_model5 = []\n",
    "for i in range(len(doc_model.docvecs)):\n",
    "    x_model5.append(doc_model.docvecs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15150775,  0.07151749,  0.07180201, ...,  0.132304  ,\n",
       "         0.0329794 , -0.08733618],\n",
       "       [ 0.0686096 , -0.01888565,  0.08451887, ..., -0.05630482,\n",
       "        -0.08765972, -0.23774159],\n",
       "       [-0.00334271, -0.00906468, -0.02458597, ...,  0.00251458,\n",
       "        -0.02325469, -0.02219841],\n",
       "       ..., \n",
       "       [ 0.1284593 , -0.1165425 , -0.13060783, ..., -0.03808872,\n",
       "        -0.05020564,  0.05547182],\n",
       "       [-0.02539518,  0.01650828,  0.01879966, ...,  0.0625739 ,\n",
       "         0.02941871, -0.15449521],\n",
       "       [ 0.0593664 , -0.02316635, -0.04938976, ...,  0.02264089,\n",
       "        -0.01537439, -0.03399729]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_model5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(x_model5), y, test_size=0.1, random_state=2)\n",
    "partial_x_train, x_val, partial_y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 32)                6432      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 6,597\n",
      "Trainable params: 6,597\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build first nerual network\n",
    "model5 = Sequential()\n",
    "\n",
    "model5.add(Dense(32, activation = 'relu', input_shape = (200,)))\n",
    "\n",
    "model5.add(Dropout(0.2))\n",
    "\n",
    "model5.add(Dense(5, activation='sigmoid'))\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.compile(optimizer='rmsprop', \n",
    "         loss='binary_crossentropy', \n",
    "               \n",
    "         metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53219 samples, validate on 5914 samples\n",
      "Epoch 1/20\n",
      "53219/53219 [==============================] - 0s 5us/step - loss: 0.5404 - acc: 0.7866 - val_loss: 0.4702 - val_acc: 0.8009\n",
      "Epoch 2/20\n",
      "53219/53219 [==============================] - 0s 4us/step - loss: 0.4657 - acc: 0.8002 - val_loss: 0.4516 - val_acc: 0.8031\n",
      "Epoch 3/20\n",
      "53219/53219 [==============================] - 0s 4us/step - loss: 0.4528 - acc: 0.8020 - val_loss: 0.4423 - val_acc: 0.8066\n",
      "Epoch 4/20\n",
      "53219/53219 [==============================] - 0s 4us/step - loss: 0.4453 - acc: 0.8046 - val_loss: 0.4369 - val_acc: 0.8085\n",
      "Epoch 5/20\n",
      "53219/53219 [==============================] - 0s 4us/step - loss: 0.4416 - acc: 0.8052 - val_loss: 0.4339 - val_acc: 0.8089\n",
      "Epoch 6/20\n",
      "53219/53219 [==============================] - 0s 4us/step - loss: 0.4391 - acc: 0.8056 - val_loss: 0.4318 - val_acc: 0.8092\n",
      "Epoch 7/20\n",
      "53219/53219 [==============================] - 0s 4us/step - loss: 0.4374 - acc: 0.8065 - val_loss: 0.4302 - val_acc: 0.8102\n",
      "Epoch 8/20\n",
      "53219/53219 [==============================] - 0s 4us/step - loss: 0.4363 - acc: 0.8071 - val_loss: 0.4288 - val_acc: 0.8106\n",
      "Epoch 9/20\n",
      "53219/53219 [==============================] - 0s 4us/step - loss: 0.4349 - acc: 0.8074 - val_loss: 0.4280 - val_acc: 0.8111\n",
      "Epoch 10/20\n",
      "53219/53219 [==============================] - 0s 4us/step - loss: 0.4343 - acc: 0.8077 - val_loss: 0.4270 - val_acc: 0.8110\n",
      "Epoch 11/20\n",
      "53219/53219 [==============================] - 0s 3us/step - loss: 0.4335 - acc: 0.8079 - val_loss: 0.4262 - val_acc: 0.8116\n",
      "Epoch 12/20\n",
      "53219/53219 [==============================] - 0s 4us/step - loss: 0.4325 - acc: 0.8086 - val_loss: 0.4256 - val_acc: 0.8114\n",
      "Epoch 13/20\n",
      "53219/53219 [==============================] - 0s 4us/step - loss: 0.4318 - acc: 0.8082 - val_loss: 0.4250 - val_acc: 0.8121\n",
      "Epoch 14/20\n",
      "53219/53219 [==============================] - 0s 4us/step - loss: 0.4313 - acc: 0.8085 - val_loss: 0.4242 - val_acc: 0.8117\n",
      "Epoch 15/20\n",
      "53219/53219 [==============================] - 0s 4us/step - loss: 0.4305 - acc: 0.8087 - val_loss: 0.4234 - val_acc: 0.8123\n",
      "Epoch 16/20\n",
      "53219/53219 [==============================] - 0s 4us/step - loss: 0.4301 - acc: 0.8091 - val_loss: 0.4226 - val_acc: 0.8127\n",
      "Epoch 17/20\n",
      "53219/53219 [==============================] - 0s 4us/step - loss: 0.4295 - acc: 0.8089 - val_loss: 0.4222 - val_acc: 0.8120\n",
      "Epoch 18/20\n",
      "53219/53219 [==============================] - 0s 4us/step - loss: 0.4293 - acc: 0.8094 - val_loss: 0.4216 - val_acc: 0.8124\n",
      "Epoch 19/20\n",
      "53219/53219 [==============================] - 0s 4us/step - loss: 0.4286 - acc: 0.8092 - val_loss: 0.4213 - val_acc: 0.8123\n",
      "Epoch 20/20\n",
      "53219/53219 [==============================] - 0s 4us/step - loss: 0.4280 - acc: 0.8095 - val_loss: 0.4209 - val_acc: 0.8121\n"
     ]
    }
   ],
   "source": [
    "result5 = model5.fit(partial_x_train, partial_y_train, epochs=20,\n",
    "                   batch_size = 512, validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81059204440849086"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.evaluate(X_test, y_test, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compare with other machine learing algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "svc = SVC()\n",
    "lr = LogisticRegression()\n",
    "clf = MultinomialNB(alpha = 0.01) \n",
    "rf = RandomForestClassifier()\n",
    "ada = AdaBoostClassifier()\n",
    "X =  tf_vec \n",
    "y_mach= data.label_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lr', 0.57322458961829026), ('rf', 0.51079419965764472), ('clf', 0.47959253998347667), ('ada', 0.46123799223410333), ('svc', 0.38416230503245924)]\n"
     ]
    }
   ],
   "source": [
    "models = [# use tf_idf as input\n",
    "    (\"lr\", lr),\n",
    "    (\"svc\", svc),\n",
    "    (\"rf\", rf),\n",
    "    (\"clf\", clf),\n",
    "    (\"ada\", ada)\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "unsorted_scores = [(name, cross_val_score(model, X, y_mach, cv=5).mean()) for name, model in models]\n",
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])\n",
    "\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ada</th>\n",
       "      <th>clf</th>\n",
       "      <th>lr</th>\n",
       "      <th>rf</th>\n",
       "      <th>svc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.461238</td>\n",
       "      <td>0.479593</td>\n",
       "      <td>0.573225</td>\n",
       "      <td>0.510794</td>\n",
       "      <td>0.384162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ada       clf        lr        rf       svc\n",
       "0  0.461238  0.479593  0.573225  0.510794  0.384162"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dict(scores),index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ada', 0.42650407400968549), ('lr', 0.39824053777778212), ('svc', 0.38416230503245924), ('rf', 0.38244368166579801)]\n",
      "Wall time: 1h 40min 47s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "models = [# use doc2vec as input\n",
    "    (\"lr\", lr),\n",
    "    (\"svc\", svc),\n",
    "    (\"rf\", rf),\n",
    "    \n",
    "    (\"ada\", ada)\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "unsorted_scores = [(name, cross_val_score(model, np.array(x_model5), y_mach, cv=5).mean()) for name, model in models]\n",
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])\n",
    "\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ada</th>\n",
       "      <th>lr</th>\n",
       "      <th>rf</th>\n",
       "      <th>svc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.426504</td>\n",
       "      <td>0.398241</td>\n",
       "      <td>0.382444</td>\n",
       "      <td>0.384162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ada        lr        rf       svc\n",
       "0  0.426504  0.398241  0.382444  0.384162"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dict(scores),index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### summary:\n",
    "deep learing model have high performance in this model\n",
    "the parameter tuning in deep learing model is very important, because there are no grid search in keras and for avoiding repeating code, I just keep the parameter that show the best result.\n",
    "add word2vec not increase the performance of this model much, but the model with word2vec and doc2vec have low overfitting problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
